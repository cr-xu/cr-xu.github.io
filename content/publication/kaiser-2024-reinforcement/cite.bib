@article{kaiser2024reinforcement,
 author = {Kaiser, Jan and Xu, Chenran and Eichler, Annika and Santamaria Garcia, Andrea and Stein, Oliver and Br√ºndermann, Erik and Kuropka, Willi and Dinter, Hannes and Mayet, Frank and Vinatier, Thomas and Burkart, Florian and Schlarb, Holger},
 da = {2024/07/08},
 doi = {10.1038/s41598-024-66263-y},
 isbn = {2045-2322},
 journal = {Scientific Reports},
 number = {1},
 pages = {15733},
 title = {Reinforcement learning-trained optimisers and Bayesian optimisation for online particle accelerator tuning},
 url = {https://doi.org/10.1038/s41598-024-66263-y},
 volume = {14},
 year = {2024},
 abstract = {Online tuning of particle accelerators is a complex optimisation problem that continues to require manual intervention by experienced human operators. Autonomous tuning is a rapidly expanding field of research, where learning-based methods like Bayesian optimisation (BO) hold great promise in improving plant performance and reducing tuning times. At the same time, reinforcement learning (RL) is a capable method of learning intelligent controllers, and recent work shows that RL can also be used to train domain-specialised optimisers in so-called reinforcement learning-trained optimisation (RLO). In parallel efforts, both algorithms have found successful adoption in particle accelerator tuning. Here we present a comparative case study, assessing the performance of both algorithms while providing a nuanced analysis of the merits and the practical challenges involved in deploying them to real-world facilities. Our results will help practitioners choose a suitable learning-based tuning algorithm for their tuning tasks, accelerating the adoption of autonomous tuning algorithms, ultimately improving the availability of particle accelerators and pushing their operational limits.},
}

